import torch

qwen_config = {
    "act_fn": "silu",
    "attention_dir": "causal",
    "attn_only": False,
    "attn_scale": 11.313708498984761,
    "attn_scores_soft_cap": -1.0,
    "attn_types": None,
    "checkpoint_index": None,
    "checkpoint_label_type": None,
    "checkpoint_value": None,
    "d_head": 128,
    "d_mlp": 8960,
    "d_model": 1536,
    "d_vocab": 151936,
    "d_vocab_out": 151936,
    "decoder_start_token_id": None,
    "default_prepend_bos": True,
    "dtype": torch.float32,
    "eps": 1e-06,
    "experts_per_token": None,
    "final_rms": True,
    "from_checkpoint": False,
    "gated_mlp": True,
    "init_mode": "gpt2",
    "init_weights": False,
    "initializer_range": 0.02,
    "load_in_4bit": False,
    "model_name": "Qwen2-1.5B",
    "n_ctx": 2048,
    "n_devices": 1,
    "n_heads": 12,
    "n_key_value_heads": 2,
    "n_layers": 28,
    "n_params": 1420296192,
    "normalization_type": "RMSPre",
    "num_experts": None,
    "original_architecture": "Qwen2ForCausalLM",
    "output_logits_soft_cap": -1.0,
    "parallel_attn_mlp": False,
    "positional_embedding_type": "rotary",
    "post_embedding_ln": False,
    "relative_attention_max_distance": None,
    "relative_attention_num_buckets": None,
    "rotary_adjacent_pairs": False,
    "rotary_base": 1000000.0,
    "rotary_dim": 128,
    "scale_attn_by_inverse_layer_idx": False,
    "seed": None,
    "tie_word_embeddings": False,
    "tokenizer_name": "Qwen/Qwen2-1.5B",
    "tokenizer_prepends_bos": False,
    "trust_remote_code": False,
    "use_attn_in": False,
    "use_attn_result": False,
    "use_attn_scale": True,
    "use_hook_mlp_in": False,
    "use_hook_tokens": False,
    "use_local_attn": False,
    "use_normalization_before_and_after": False,
    "use_split_qkv_input": False,
    "window_size": None,
}


qwen_config_tiny = {
    "act_fn": "silu",
    "attention_dir": "causal",
    "attn_only": False,
    "attn_scale": 11.313708498984761,
    "attn_scores_soft_cap": -1.0,
    "attn_types": None,
    "checkpoint_index": None,
    "checkpoint_label_type": None,
    "checkpoint_value": None,
    "d_head": 4,
    "d_mlp": 10,
    "d_model": 24,
    "d_vocab": 20,
    "d_vocab_out": 20,
    "decoder_start_token_id": None,
    "default_prepend_bos": True,
    "dtype": torch.float32,
    "eps": 1e-06,
    "experts_per_token": None,
    "final_rms": True,
    "from_checkpoint": False,
    "gated_mlp": True,
    "init_mode": "gpt2",
    "init_weights": False,
    "initializer_range": 0.02,
    "load_in_4bit": False,
    "model_name": "Qwen2-1.5B",
    "n_ctx": 2048,
    "n_devices": 1,
    "n_heads": 6,
    "n_key_value_heads": 2,
    "n_layers": 3,
    "normalization_type": "RMSPre",
    "num_experts": None,
    "output_logits_soft_cap": -1.0,
    "parallel_attn_mlp": False,
    "positional_embedding_type": "rotary",
    "post_embedding_ln": False,
    "relative_attention_max_distance": None,
    "relative_attention_num_buckets": None,
    "rotary_adjacent_pairs": False,
    "rotary_base": 1000000.0,
    "rotary_dim": 4,
    "scale_attn_by_inverse_layer_idx": False,
    "seed": None,
    "tie_word_embeddings": False,
    "tokenizer_name": "Qwen/Qwen2-1.5B",
    "tokenizer_prepends_bos": False,
    "trust_remote_code": False,
    "use_attn_in": False,
    "use_attn_result": False,
    "use_attn_scale": True,
    "use_hook_mlp_in": False,
    "use_hook_tokens": False,
    "use_local_attn": False,
    "use_normalization_before_and_after": False,
    "use_split_qkv_input": False,
    "window_size": None,
}
